{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad306f1-c635-4863-b5f9-c6a155ddb610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/13 09:24:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Create a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Note that we have set parallelism to 8\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"OptimizeProcessingJob\")\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", 8)\\\n",
    "            .config(\"spark.default.parallelism\", 8)\\\n",
    "            .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .master(\"local[2]\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d5bc1-60e7-4a06-a5d7-3db0167689d0",
   "metadata": {},
   "source": [
    "### 05.01 Pushing down Projections\n",
    "When downstream queries/processing only looks for a subset of columns, Spark optimizer is smart enough to identify them and only read those columns into the in-memory data frame. This saves on I/O and memory. This is called Projection Push down. While building data pipelines, it helps to be aware of how Spark works and take advantage of this for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78625334-1182-49d0-9bfe-4ea302f823cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------EXPLAIN--------------------------\n",
      "== Physical Plan ==\n",
      "*(1) Project [Product#6, Quantity#3]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [Quantity#3,Product#6] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/linkedin/ExerciseFiles/dummy_hdfs/partitioned_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Quantity:int>\n",
      "\n",
      "\n",
      "-------------------------END EXPLAIN-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_data = spark\\\n",
    "                .read\\\n",
    "                .parquet(\"dummy_hdfs/partitioned_parquet\")\n",
    "\n",
    "#show the execution plan\n",
    "print(\"\\n--------------------------EXPLAIN--------------------------\")\n",
    "sales_data.select(\"Product\",\"Quantity\").explain()\n",
    "print(\"-------------------------END EXPLAIN-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb431c-9fb9-4926-9865-a4f1bd367dce",
   "metadata": {},
   "source": [
    "### 05.02 Pushing down Filters\n",
    "When downstream queries/processing only looks for a subset of subset, Spark optimizer is smart enough to identify them and only read those columns into the in-memory data frame. This saves on I/O and memory. This is called Filter Push down. This works for both partition columns and non-partition columns. While building data pipelines, it helps to be aware of how Spark works and take advantage of this for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813ced08-24b0-420a-9fc4-3e267e9ce12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+--------+-----+--------------------+-------+\n",
      "| ID|Customer|      Date|Quantity| Rate|                Tags|Product|\n",
      "+---+--------+----------+--------+-----+--------------------+-------+\n",
      "|  6|  Google|2019/11/23|       5|40.58|                NULL|  Mouse|\n",
      "|  8|  Google|2019/11/13|       1|46.79|Urgent:Discount:P...|  Mouse|\n",
      "| 14|   Apple|2019/11/09|       4|40.27|            Discount|  Mouse|\n",
      "| 15|   Apple|2019/11/25|       5|38.89|                NULL|  Mouse|\n",
      "| 20|LinkedIn|2019/11/25|       4|36.77|       Urgent:Pickup|  Mouse|\n",
      "+---+--------+----------+--------+-----+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "--------------------------EXPLAIN--------------------------\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [ID#0,Customer#1,Date#2,Quantity#3,Rate#4,Tags#5,Product#6] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/linkedin/ExerciseFiles/dummy_hdfs/partitioned_parquet], PartitionFilters: [isnotnull(Product#6), (Product#6 = Mouse)], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
      "\n",
      "\n",
      "-------------------------END EXPLAIN-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Use a partition attribute for filtering\n",
    "mouse_df = sales_data.where(col(\"Product\") == 'Mouse')\n",
    "mouse_df.show(5)\n",
    "\n",
    "#show the execution plan\n",
    "print(\"\\n--------------------------EXPLAIN--------------------------\")\n",
    "mouse_df.explain()\n",
    "print(\"-------------------------END EXPLAIN-----------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e84d08-5d3d-4c57-a9c0-20ca5108207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+--------+-----+--------------------+-------+\n",
      "| ID|Customer|      Date|Quantity| Rate|                Tags|Product|\n",
      "+---+--------+----------+--------+-----+--------------------+-------+\n",
      "|  6|  Google|2019/11/23|       5|40.58|                NULL|  Mouse|\n",
      "|  8|  Google|2019/11/13|       1|46.79|Urgent:Discount:P...|  Mouse|\n",
      "| 35|  Google|2019/11/17|       2|49.33|              Pickup|  Mouse|\n",
      "| 51|  Google|2019/11/27|       4| 32.8|              Urgent|  Mouse|\n",
      "| 57|  Google|2019/11/21|       5| 32.0|              Pickup|  Mouse|\n",
      "+---+--------+----------+--------+-----+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "--------------------------EXPLAIN--------------------------\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(Customer#1) AND (Customer#1 = Google))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [ID#0,Customer#1,Date#2,Quantity#3,Rate#4,Tags#5,Product#6] Batched: true, DataFilters: [isnotnull(Customer#1), (Customer#1 = Google)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/linkedin/ExerciseFiles/dummy_hdfs/partitioned_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(Customer), EqualTo(Customer,Google)], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
      "\n",
      "\n",
      "-------------------------END EXPLAIN-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "google_df = sales_data.where(col(\"Customer\") == 'Google')\n",
    "google_df.show(5)\n",
    "\n",
    "#show the execution plan\n",
    "print(\"\\n--------------------------EXPLAIN--------------------------\")\n",
    "google_df.explain()\n",
    "print(\"-------------------------END EXPLAIN-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620084a4-9353-4134-af90-7c813ccbb301",
   "metadata": {},
   "source": [
    "### 05.03 Partitioning and coalescing\n",
    "While performing actions, Spark creates results with the default partition count. In the case of Local mode, its usually equal to the number of cores. In the case of Clusters, the default is 200. This can be too much, if the number of cores in the cluster is significantly less than the number of partitions. So repartitioning helps to set the optimal number of partitions.\n",
    "\n",
    "Repartition does a full reshuffle and can be used for increasing/decreasing partitions.\n",
    "\n",
    "Coalasce simply consolidates existing partitions and avoids a full reshuffle. It can be used to decrease the number of partitions.\n",
    "\n",
    "Repartition and Coalasce themselves take significant time and resources. Do them only if multiple steps downstream will benefit from them."
   ]
  },
  {
   "cell_type": "raw",
   "id": "af6078f1-8e0d-418f-ab59-1297137c6a0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df68e40-a78c-4f45-b405-6181324febe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parallelism from Spark Session : 8\n",
      "\n",
      "Partitions in hdfs data with parquet :  4\n",
      "\n",
      "Partitions in data frame for raw CSV read :  1\n",
      "\n",
      "Partitions in raw data frame after repartitioning :  5\n",
      "\n",
      "Partitions in raw data frame after coalesce :  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Default parallelism from Spark Session :\", \n",
    "      spark.sparkContext.defaultParallelism)\n",
    "\n",
    "#Read a file with default parallelism\n",
    "raw_sales_data = spark\\\n",
    "                .read\\\n",
    "                .option(\"inferSchema\", \"true\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(\"datasets/sales_orders.csv\")\n",
    "\n",
    "#Partitions in sales data partitioned by product (read previously)\n",
    "#1 partition per product\n",
    "print(\"\\nPartitions in hdfs data with parquet : \",\n",
    "      sales_data.rdd.getNumPartitions())\n",
    "\n",
    "#Raw partition count\n",
    "print(\"\\nPartitions in data frame for raw CSV read : \", \n",
    "      raw_sales_data.rdd.getNumPartitions())\n",
    "\n",
    "#Repartition to 5 partitions\n",
    "partitioned_sales_data = raw_sales_data.repartition(5)\n",
    "\n",
    "print(\"\\nPartitions in raw data frame after repartitioning : \", \n",
    "      partitioned_sales_data.rdd.getNumPartitions())\n",
    "\n",
    "#coalesce to 3 partitions\n",
    "coalesced_sales_data = partitioned_sales_data.coalesce(3)\n",
    "\n",
    "print(\"\\nPartitions in raw data frame after coalesce : \", \n",
    "      coalesced_sales_data.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc05e6-9f91-49bf-97da-11d37ebfed80",
   "metadata": {},
   "source": [
    "### 05.04 Optimizing Joins\n",
    "By default, joining two data frames require a lot of shuffling. If one data frame is considerably small, a better option is to broadcast that data frame to all the executors and then use those copies to join locally. Spark Optimizer chooses Broadcast joins when possible. Data frames within spark.sql.autoBroadcastJoinThreshold are automatically broadcasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d951b37-eba6-45b1-bb43-d7d3fa31352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "| Product|     Vendor|\n",
      "+--------+-----------+\n",
      "|   Mouse|   Logitech|\n",
      "|Keyboard|  Microsoft|\n",
      "|  Webcam|   Logitech|\n",
      "| Headset|Plantronics|\n",
      "+--------+-----------+\n",
      "\n",
      "+-------+---+--------+----------+--------+-----+--------------------+--------+\n",
      "|Product| ID|Customer|      Date|Quantity| Rate|                Tags|  Vendor|\n",
      "+-------+---+--------+----------+--------+-----+--------------------+--------+\n",
      "|  Mouse|  6|  Google|2019/11/23|       5|40.58|                NULL|Logitech|\n",
      "|  Mouse|  8|  Google|2019/11/13|       1|46.79|Urgent:Discount:P...|Logitech|\n",
      "|  Mouse| 14|   Apple|2019/11/09|       4|40.27|            Discount|Logitech|\n",
      "|  Mouse| 15|   Apple|2019/11/25|       5|38.89|                NULL|Logitech|\n",
      "|  Mouse| 20|LinkedIn|2019/11/25|       4|36.77|       Urgent:Pickup|Logitech|\n",
      "+-------+---+--------+----------+--------+-----+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "--------------------------EXPLAIN--------------------------\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [Product#6, ID#0, Customer#1, Date#2, Quantity#3, Rate#4, Tags#5, Vendor#145]\n",
      "   +- BroadcastHashJoin [Product#6], [Product#144], Inner, BuildRight, false\n",
      "      :- FileScan parquet [ID#0,Customer#1,Date#2,Quantity#3,Rate#4,Tags#5,Product#6] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/linkedin/ExerciseFiles/dummy_hdfs/partitioned_parquet], PartitionFilters: [isnotnull(Product#6)], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=251]\n",
      "         +- Filter isnotnull(Product#144)\n",
      "            +- FileScan csv [Product#144,Vendor#145] Batched: false, DataFilters: [isnotnull(Product#144)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/linkedin/ExerciseFiles/datasets/product_vendor.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Product)], ReadSchema: struct<Product:string,Vendor:string>\n",
      "\n",
      "\n",
      "-------------------------END EXPLAIN-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "product_data = spark\\\n",
    "                .read\\\n",
    "                .option(\"inferSchema\", \"true\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(\"datasets/product_vendor.csv\")\n",
    "product_data.show(5)\n",
    "\n",
    "#Broadcast product data\n",
    "broadcast_product=broadcast(product_data)\n",
    "\n",
    "#Join with broadcasted local copy of product data\n",
    "joined_data = sales_data.join(broadcast_product,\"Product\")\n",
    "\n",
    "joined_data.show(5)\n",
    "\n",
    "#show the execution plan\n",
    "print(\"\\n--------------------------EXPLAIN--------------------------\")\n",
    "joined_data.explain()\n",
    "print(\"-------------------------END EXPLAIN-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9852d-1786-4e06-b06f-32afda0cee94",
   "metadata": {},
   "source": [
    "### 05.05 Storing Intermediate Results\n",
    "By default, every time an action is performed, Spark executes all the previous steps right from the data read. This can end up being very expensive, especially while using Spark in a development or interactive mode. A better option is to cache intermediate results. Spark can cache in memory. It can also persist in both memory and disk. While running under YARN, persistance happens in HDFS by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9caaa060-b11a-4b72-af80-3e00e33bc84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan before caching intermediate results:\n",
      "-------------------------\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Coalesce 3\n",
      "   +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=270]\n",
      "      +- Filter (isnotnull(Customer#93) AND (Customer#93 = Google))\n",
      "         +- FileScan csv [ID#92,Customer#93,Product#94,Date#95,Quantity#96,Rate#97,Tags#98] Batched: false, DataFilters: [isnotnull(Customer#93), (Customer#93 = Google)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/linkedin/ExerciseFiles/datasets/sales_orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Customer), EqualTo(Customer,Google)], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
      "\n",
      "\n",
      "Plan after caching intermediate results:\n",
      "-------------------------\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Filter (isnotnull(Customer#93) AND (Customer#93 = Google))\n",
      "   +- InMemoryTableScan [ID#92, Customer#93, Product#94, Date#95, Quantity#96, Rate#97, Tags#98], [isnotnull(Customer#93), (Customer#93 = Google)]\n",
      "         +- InMemoryRelation [ID#92, Customer#93, Product#94, Date#95, Quantity#96, Rate#97, Tags#98], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                  +- Coalesce 3\n",
      "                     +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=281]\n",
      "                        +- FileScan csv [ID#92,Customer#93,Product#94,Date#95,Quantity#96,Rate#97,Tags#98] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/linkedin/ExerciseFiles/datasets/sales_orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Plan before caching intermediate results:\\n-------------------------\")\n",
    "data_before = coalesced_sales_data.where(col(\"Customer\") == 'Google')\n",
    "data_before.explain()\n",
    "\n",
    "#store intermediate results on disk\n",
    "coalesced_sales_data.persist()\n",
    "\n",
    "print(\"Plan after caching intermediate results:\\n-------------------------\")\n",
    "data_after = coalesced_sales_data.where(col(\"Customer\") == 'Google')\n",
    "data_after.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d732932-cb9a-4883-bc07-c26a6ab948c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
