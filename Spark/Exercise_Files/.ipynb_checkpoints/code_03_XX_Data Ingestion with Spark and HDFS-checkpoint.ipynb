{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9bc44b-09a6-4541-8b39-b47bd0d4f4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/12 14:16:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "#Create a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"SparkWriterJob\")\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", 2)\\\n",
    "            .config(\"spark.default.parallelism\", 2)\\\n",
    "            .master(\"local[2]\")\\\n",
    "            .getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3088741-a6f5-4492-bbff-f5ea60ea5266",
   "metadata": {},
   "source": [
    "### 03.01 Reading Files into Spark\n",
    "\n",
    "Data can be read into Apache Spark data frames from a variety of data sources. \n",
    "\n",
    "examples : \n",
    "- A flat file on a local disk\n",
    "- A file from HDFS\n",
    "- A Kafka Topic\n",
    "\n",
    "\n",
    "In this example, we will read a CSV file in a HDFS folder into a Spark Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3a9487-15dc-4c62-8b09-e6e1feaac1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Customer: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Rate: double (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      "\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "| ID|Customer| Product|      Date|Quantity| Rate|           Tags|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "|  1|   Apple|Keyboard|2019/11/21|       5|31.15|Discount:Urgent|\n",
      "|  2|LinkedIn| Headset|2019/11/25|       5| 36.9|  Urgent:Pickup|\n",
      "|  3|Facebook|Keyboard|2019/11/24|       5|49.89|           NULL|\n",
      "|  4|  Google|  Webcam|2019/11/07|       4|34.21|       Discount|\n",
      "|  5|LinkedIn|  Webcam|2019/11/21|       3|48.69|         Pickup|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the raw CSV file int a Spark DataFrame\n",
    "#    Use inferSchema to infer the schema automatically from the CSV file\n",
    "\n",
    "raw_sales_data = spark\\\n",
    "                .read\\\n",
    "                .option(\"inferSchema\", \"true\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(\"datasets/sales_orders.csv\")\n",
    "\n",
    "#Print the schema for verification\n",
    "raw_sales_data.printSchema();\n",
    "\n",
    "#Print the first 5 records for verification\n",
    "raw_sales_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c47c7-2ac6-43d3-87ec-4a78c915f69a",
   "metadata": {},
   "source": [
    "### 03.02 Writing to HDFS\n",
    "\n",
    "Write the rawSalesData Data Frame into HDFS as a Parquet file. Use Parquet as the format since it enables splitting and filtering. Use GZIP as the compression codec. \n",
    "\n",
    "On completion, verify if the files are correctly through the filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "975d23e7-5dba-41bb-9335-58ab11190d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sales_data.write\\\n",
    "            .option(\"compression\", \"gzip\")\\\n",
    "            .parquet(path=\"dummy_hdfs/raw_parquet\",\n",
    "                    mode=\"overwrite\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2aff0-68ca-411c-aaff-359ed04c6f2f",
   "metadata": {},
   "source": [
    "### 03.03 Write to HDFS with partitioning\n",
    "\n",
    "Write a partitioned Parquet file in HDFS. Partition will be done by Product. This will create one directory per unique product available in the raw CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d02fc4-12cc-4b1b-b53d-2a24a44594c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sales_data.write\\\n",
    "            .option(\"compression\", \"gzip\")\\\n",
    "            .partitionBy(\"Product\")\\\n",
    "            .parquet(path=\"dummy_hdfs/partitioned_parquet\",\n",
    "                    mode=\"overwrite\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ca0df-4dab-4ad8-b4a9-df540df64749",
   "metadata": {},
   "source": [
    "### 03.04 Writing to Hive with Bucketing\n",
    "\n",
    "Create a Bucketed Hive table for orders. Bucketing will be done by Product. It will create 3 buckets based on the hash generated by Product. Hive tables can be queried through SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349bea20-92ea-4c04-adfc-edf50625da58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|product_bucket_table|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n",
      "+---+--------+-------+----------+--------+-----+--------------------+\n",
      "| ID|Customer|Product|      Date|Quantity| Rate|                Tags|\n",
      "+---+--------+-------+----------+--------+-----+--------------------+\n",
      "|  6|  Google|  Mouse|2019/11/23|       5|40.58|                NULL|\n",
      "|  8|  Google|  Mouse|2019/11/13|       1|46.79|Urgent:Discount:P...|\n",
      "| 14|   Apple|  Mouse|2019/11/09|       4|40.27|            Discount|\n",
      "| 15|   Apple|  Mouse|2019/11/25|       5|38.89|                NULL|\n",
      "| 20|LinkedIn|  Mouse|2019/11/25|       4|36.77|       Urgent:Pickup|\n",
      "+---+--------+-------+----------+--------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Make sure that the \n",
    "raw_sales_data.write\\\n",
    "            .format(\"parquet\")\\\n",
    "            .bucketBy(3, \"Product\")\\\n",
    "            .saveAsTable(\"product_bucket_table\")\n",
    "            \n",
    "#Spark Hive table is stored in spark-warehouse folder\n",
    "\n",
    "spark.sql(\"SHOW tables\").show(5)\n",
    "\n",
    "#Read bucketed data\n",
    "spark.sql(f\"\"\"\n",
    "        SELECT * FROM product_bucket_table \n",
    "        WHERE Product='Mouse'\"\"\")\\\n",
    "    .show(5)\n",
    "#While the files are persisted to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c3463-0f83-415e-8fb3-4a431dedc931",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4d3ca-27a9-433b-975d-10e83a9f4926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
