{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a4cfe3-b04e-46c3-86d3-db9f28cd44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"SparkReadJob\")\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", 2)\\\n",
    "            .config(\"spark.default.parallelism\", 2)\\\n",
    "            .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .master(\"local[2]\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bafba95-4e52-4be0-a657-14eff633e258",
   "metadata": {},
   "source": [
    "### 04.02 Read Parquet Files into Spark\n",
    "Read a non-partitioned Parquet file into Spark. Measure the time taken. Also look at the execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b586e0-1b2e-41ed-ae37-fbb93af4eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "| ID|Customer| Product|      Date|Quantity| Rate|           Tags|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "|  1|   Apple|Keyboard|2019/11/21|       5|31.15|Discount:Urgent|\n",
      "|  2|LinkedIn| Headset|2019/11/25|       5| 36.9|  Urgent:Pickup|\n",
      "|  3|Facebook|Keyboard|2019/11/24|       5|49.89|           NULL|\n",
      "|  4|  Google|  Webcam|2019/11/07|       4|34.21|       Discount|\n",
      "|  5|LinkedIn|  Webcam|2019/11/21|       3|48.69|         Pickup|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "--------------------------EXPLAIN--------------------------\n",
      "== Parsed Logical Plan ==\n",
      "Relation [ID#0,Customer#1,Product#2,Date#3,Quantity#4,Rate#5,Tags#6] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ID: int, Customer: string, Product: string, Date: string, Quantity: int, Rate: double, Tags: string\n",
      "Relation [ID#0,Customer#1,Product#2,Date#3,Quantity#4,Rate#5,Tags#6] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [ID#0,Customer#1,Product#2,Date#3,Quantity#4,Rate#5,Tags#6] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [ID#0,Customer#1,Product#2,Date#3,Quantity#4,Rate#5,Tags#6] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/D:/DS/Big_Data_Analytics/Spark/Big_Data_Analytics_Hadoop_Apache_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
      "\n",
      "-------------------------END EXPLAIN-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_parquet = spark\\\n",
    "                .read\\\n",
    "                .parquet(\"dummy_hdfs/raw_parquet\")\n",
    "\n",
    "#Display the results\n",
    "sales_parquet.show(5)\n",
    "\n",
    "#show the execution plan\n",
    "print(\"\\n--------------------------EXPLAIN--------------------------\")\n",
    "sales_parquet.explain(True)\n",
    "print(\"-------------------------END EXPLAIN-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d0a8d-0874-49f4-97e1-b6a9992fff31",
   "metadata": {},
   "source": [
    "### 04.03. Read Partitioned Data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41627f71-5748-4aa4-8e43-93a3ac51daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+--------+-----+--------------------+\n",
      "| ID|Customer|      Date|Quantity| Rate|                Tags|\n",
      "+---+--------+----------+--------+-----+--------------------+\n",
      "|  6|  Google|2019/11/23|       5|40.58|                NULL|\n",
      "|  8|  Google|2019/11/13|       1|46.79|Urgent:Discount:P...|\n",
      "| 14|   Apple|2019/11/09|       4|40.27|            Discount|\n",
      "| 15|   Apple|2019/11/25|       5|38.89|                NULL|\n",
      "| 20|LinkedIn|2019/11/25|       4|36.77|       Urgent:Pickup|\n",
      "+---+--------+----------+--------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "--------------------------EXPLAIN--------------------------\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [ID#44,Customer#45,Date#46,Quantity#47,Rate#48,Tags#49] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(5 paths)[file:/D:/DS/Big_Data_Analytics/Spark/Big_Data_Analytics_Hadoop_Apache_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
      "\n",
      "\n",
      "-------------------------END EXPLAIN-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_partitioned = spark\\\n",
    "                    .read\\\n",
    "                    .parquet(\"dummy_hdfs/partitioned_parquet/*\")\n",
    "\n",
    "#Display the results\n",
    "sales_partitioned.show(5)\n",
    "\n",
    "#show the execution plan\n",
    "print(\"\\n--------------------------EXPLAIN--------------------------\")\n",
    "sales_partitioned.explain()\n",
    "print(\"-------------------------END EXPLAIN-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45899b52-23cc-4588-bb64-96d055f47fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+--------+-----+--------------------+\n",
      "| ID|Customer|      Date|Quantity| Rate|                Tags|\n",
      "+---+--------+----------+--------+-----+--------------------+\n",
      "|  2|LinkedIn|2019/11/25|       5| 36.9|       Urgent:Pickup|\n",
      "| 10|LinkedIn|2019/11/09|       2|26.91|Urgent:Discount:P...|\n",
      "| 11|Facebook|2019/11/26|       5|45.84|       Urgent:Pickup|\n",
      "| 12|  Google|2019/11/05|       2|41.17|     Discount:Urgent|\n",
      "| 17|   Apple|2019/11/09|       4|29.98|     Discount:Urgent|\n",
      "+---+--------+----------+--------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read specific partition only\n",
    "sales_headset = spark\\\n",
    "                    .read\\\n",
    "                    .parquet(\"dummy_hdfs/partitioned_parquet/Product=Headset\")\n",
    "sales_headset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd802eb-2b9e-4413-aba9-dbf0a105fcec",
   "metadata": {},
   "source": [
    "### 04.04 Read Bucketed Data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86dfacb8-abdf-4e62-a744-7aa9e66aaa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "| ID|Customer| Product|      Date|Quantity| Rate|           Tags|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "|  1|   Apple|Keyboard|2019/11/21|       5|31.15|Discount:Urgent|\n",
      "|  3|Facebook|Keyboard|2019/11/24|       5|49.89|           NULL|\n",
      "|  4|  Google|  Webcam|2019/11/07|       4|34.21|       Discount|\n",
      "|  5|LinkedIn|  Webcam|2019/11/21|       3|48.69|         Pickup|\n",
      "|  7|LinkedIn|  Webcam|2019/11/20|       4|37.19|           NULL|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------+-------+----------+--------+-----+---------------+\n",
      "| ID|Customer|Product|      Date|Quantity| Rate|           Tags|\n",
      "+---+--------+-------+----------+--------+-----+---------------+\n",
      "|  4|  Google| Webcam|2019/11/07|       4|34.21|       Discount|\n",
      "|  5|LinkedIn| Webcam|2019/11/21|       3|48.69|         Pickup|\n",
      "|  7|LinkedIn| Webcam|2019/11/20|       4|37.19|           NULL|\n",
      "|  9|  Google| Webcam|2019/11/10|       4|27.48|Discount:Urgent|\n",
      "| 18|LinkedIn| Webcam|2019/11/06|       3|40.59|       Discount|\n",
      "+---+--------+-------+----------+--------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Spark does not persist the Hive catalog between multiple Sparksession instances\n",
    "#You can additionally use a Hive metastore if you want to persist catalog\n",
    "#across SparkSession instances\n",
    "\n",
    "#Read the bucketed table directly from disk\n",
    "sales_bucketed = spark\\\n",
    "                    .read\\\n",
    "                    .parquet(\"spark-warehouse/product_bucket_table/*\")\n",
    "\n",
    "sales_bucketed.show(5)\n",
    "\n",
    "#Convert into a temporary view\n",
    "sales_bucketed.createOrReplaceTempView(\"product_bucket_table\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM product_bucket_table WHERE Product='Webcam'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30ea36-f380-417d-91aa-d012cad45675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
