{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e474f3-fc78-4851-8d75-1ef67ead6002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/13 09:30:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/13 09:30:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#Create a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Note that we have set parallelism to 8\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"LearnerExerciseJob\")\\\n",
    "            .config(\"spark.sql.shuffle.partitions\", 8)\\\n",
    "            .config(\"spark.default.parallelism\", 8)\\\n",
    "            .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .master(\"local[2]\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e8d81-6a27-4b56-8398-8a84d318ae4b",
   "metadata": {},
   "source": [
    "### 06.02 Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ff135-d192-4d6a-9db6-f0eb7413a42f",
   "metadata": {},
   "source": [
    "#### Read CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3180a609-08be-46b0-b487-7f8368db9731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Student: string (nullable = true)\n",
      " |-- Subject: string (nullable = true)\n",
      " |-- ClassScore: double (nullable = true)\n",
      " |-- TestScore: double (nullable = true)\n",
      "\n",
      "+-------+---------+----------+---------+\n",
      "|Student|  Subject|ClassScore|TestScore|\n",
      "+-------+---------+----------+---------+\n",
      "|   Katy|     Math|      0.95|     2.37|\n",
      "|   Katy|Chemistry|       0.5|     1.18|\n",
      "|   Katy|  Physics|      0.48|     1.37|\n",
      "|   Katy|  Biology|      0.75|     2.79|\n",
      "|   Mike|     Math|      0.45|     1.79|\n",
      "+-------+---------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the student score file\n",
    "raw_student_data = spark\\\n",
    "                .read\\\n",
    "                .option(\"inferSchema\", \"true\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .csv(\"datasets/student_scores.csv\")\n",
    "\n",
    "#Display schema and data\n",
    "raw_student_data.printSchema()\n",
    "raw_student_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f87ea1-6709-4a7a-8216-0a1d127fd1cb",
   "metadata": {},
   "source": [
    "#### Create partitioned HDFS Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf9ebe3-39c9-42b8-92b4-705fe286794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store as parquet file for better performance\n",
    "raw_student_data.write\\\n",
    "            .option(\"compression\", \"gzip\")\\\n",
    "            .partitionBy(\"Subject\")\\\n",
    "            .parquet(path=\"dummy_hdfs/partitioned_student\",\n",
    "                    mode=\"overwrite\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a41e9-e19f-4c79-9ecb-4761d987eb8c",
   "metadata": {},
   "source": [
    "#### Read from partitioned store to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3edd6274-f0cf-4672-8ac1-87a505a925df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions in student data : 4\n"
     ]
    }
   ],
   "source": [
    "student_data = spark\\\n",
    "                .read\\\n",
    "                .parquet(\"dummy_hdfs/partitioned_student\")\n",
    "\n",
    "print(\"Partitions in student data :\",student_data.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a6877-31eb-4aa7-aa24-ab6a14abb2a3",
   "metadata": {},
   "source": [
    "### 06.03 Total Score Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6636b1-4285-4925-b654-540e9efca60e",
   "metadata": {},
   "source": [
    "#### Compute total score by student and subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abba5aa4-b272-4273-93a1-f6d6f7f6b080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+-------+------------------+\n",
      "|Student|ClassScore|TestScore|Subject|        TotalScore|\n",
      "+-------+----------+---------+-------+------------------+\n",
      "|   Katy|      0.95|     2.37|   Math|3.3200000000000003|\n",
      "|   Mike|      0.45|     1.79|   Math|              2.24|\n",
      "|    Bob|      0.36|     2.37|   Math|              2.73|\n",
      "|   Lisa|      0.33|     2.86|   Math|              3.19|\n",
      "|   John|      0.27|      1.2|   Math|              1.47|\n",
      "+-------+----------+---------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Create new column TotalScore by adding ClassScore and TestScore\n",
    "total_score_df = student_data.withColumn(\"TotalScore\",\n",
    "                                         col(\"ClassScore\") + col(\"TestScore\"))\n",
    "total_score_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f69628c-5220-4b22-9b61-8e0bdf2e3db8",
   "metadata": {},
   "source": [
    "#### Print total score for physics for all students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a2e49a-1675-4280-be09-b9b50a443aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+-------+------------------+\n",
      "|Student|ClassScore|TestScore|Subject|        TotalScore|\n",
      "+-------+----------+---------+-------+------------------+\n",
      "|   Katy|      0.48|     1.37|Physics|              1.85|\n",
      "|   Mike|      0.34|     2.72|Physics|              3.06|\n",
      "|    Bob|      0.93|     2.89|Physics|3.8200000000000003|\n",
      "|   Lisa|      0.42|     2.34|Physics|              2.76|\n",
      "|   John|      0.82|      2.8|Physics|3.6199999999999997|\n",
      "+-------+----------+---------+-------+------------------+\n",
      "\n",
      "\n",
      "--------------------------EXPLAIN--------------------------\n",
      "== Physical Plan ==\n",
      "*(1) Project [Student#56, ClassScore#57, TestScore#58, Subject#59, (ClassScore#57 + TestScore#58) AS TotalScore#64]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [Student#56,ClassScore#57,TestScore#58,Subject#59] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/linkedin/ExerciseFiles/dummy_hdfs/partitioned_student], PartitionFilters: [isnotnull(Subject#59), (Subject#59 = Physics)], PushedFilters: [], ReadSchema: struct<Student:string,ClassScore:double,TestScore:double>\n",
      "\n",
      "\n",
      "-------------------------END EXPLAIN-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "physics_score= total_score_df.where(col(\"Subject\") == 'Physics')\n",
    "physics_score.show()\n",
    "\n",
    "#show the execution plan\n",
    "print(\"\\n--------------------------EXPLAIN--------------------------\")\n",
    "physics_score.explain()\n",
    "print(\"-------------------------END EXPLAIN-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b8c65-0240-4ae0-8628-a1b1c30e4616",
   "metadata": {},
   "source": [
    "### 06.04 Compute average total score for each student across subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d308fa-6b6e-41fd-970a-dd20dd11b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|Student|   avg(TotalScore)|\n",
      "+-------+------------------+\n",
      "|   Katy|            2.5975|\n",
      "|   Mike|             2.455|\n",
      "|   Lisa|2.3899999999999997|\n",
      "|    Bob|3.0150000000000006|\n",
      "|   John|            2.8525|\n",
      "+-------+------------------+\n",
      "\n",
      "\n",
      "--------------------------EXPLAIN--------------------------\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[Student#56], functions=[avg(TotalScore#64)])\n",
      "   +- Exchange hashpartitioning(Student#56, 8), ENSURE_REQUIREMENTS, [plan_id=188]\n",
      "      +- HashAggregate(keys=[Student#56], functions=[partial_avg(TotalScore#64)])\n",
      "         +- InMemoryTableScan [Student#56, TotalScore#64]\n",
      "               +- InMemoryRelation [Student#56, ClassScore#57, TestScore#58, Subject#59, TotalScore#64], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) Project [Student#56, ClassScore#57, TestScore#58, Subject#59, (ClassScore#57 + TestScore#58) AS TotalScore#64]\n",
      "                        +- *(1) ColumnarToRow\n",
      "                           +- FileScan parquet [Student#56,ClassScore#57,TestScore#58,Subject#59] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/linkedin/ExerciseFiles/dummy_hdfs/partitioned_student], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Student:string,ClassScore:double,TestScore:double>\n",
      "\n",
      "\n",
      "-------------------------END EXPLAIN-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cache the total score dataframe\n",
    "total_score_df.persist()\n",
    "\n",
    "avg_score_df = total_score_df\\\n",
    "                .groupBy(\"Student\")\\\n",
    "                .avg(\"TotalScore\")\n",
    "\n",
    "avg_score_df.show()\n",
    "\n",
    "#show the execution plan\n",
    "print(\"\\n--------------------------EXPLAIN--------------------------\")\n",
    "avg_score_df.explain()\n",
    "print(\"-------------------------END EXPLAIN-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4e228-b4fa-43b5-9051-6cf8fce084f7",
   "metadata": {},
   "source": [
    "### 06.05 Find top Student by Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e937a45d-ae23-41d6-a8ea-9a17ca49a98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|  Subject|          TopScore|\n",
      "+---------+------------------+\n",
      "|     Math|3.3200000000000003|\n",
      "|  Physics|3.8200000000000003|\n",
      "|Chemistry|3.1999999999999997|\n",
      "|  Biology|              3.54|\n",
      "+---------+------------------+\n",
      "\n",
      "+---------+-------+------------------+\n",
      "|  Subject|Student|          TopScore|\n",
      "+---------+-------+------------------+\n",
      "|     Math|   Katy|3.3200000000000003|\n",
      "|  Physics|    Bob|3.8200000000000003|\n",
      "|Chemistry|   John|3.1999999999999997|\n",
      "|  Biology|   Katy|              3.54|\n",
      "+---------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find top score by subject\n",
    "top_score_df = total_score_df\\\n",
    "                .groupBy(\"Subject\")\\\n",
    "                .max(\"TotalScore\")\\\n",
    "                .withColumnRenamed(\"max(TotalScore)\",\"TopScore\")\n",
    "\n",
    "top_score_df.show()\n",
    "\n",
    "#Find the student who had the top score\n",
    "top_student_df = total_score_df.alias(\"a\")\\\n",
    "                    .join(top_score_df.alias(\"b\"),\n",
    "                            (col(\"b.TopScore\") == col(\"a.TotalScore\")) & \n",
    "                              (col(\"b.Subject\") == col(\"a.Subject\"))) \\\n",
    "                    .select(\"a.Subject\", \"a.Student\", \"b.TopScore\")\n",
    "\n",
    "top_student_df.show()\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91582369-b02c-4182-83fc-fe8072e2a166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
